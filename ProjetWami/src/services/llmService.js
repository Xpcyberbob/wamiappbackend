import Constants from 'expo-constants';
import { Platform } from 'react-native';
import { getCurrentLanguage, LANGUAGES, getSystemMessage } from './languageService';

// Configuration du LLM - Google Gemini (Principal, Gratuit)
const GEMINI_API_KEY = Constants.expoConfig?.extra?.EXPO_PUBLIC_GEMINI_API_KEY 
  || process.env.EXPO_PUBLIC_GEMINI_API_KEY;
const GEMINI_API_URL = 'https://generativelanguage.googleapis.com/v1beta/models';

// Backup 1 : Groq (Gratuit)
const GROQ_API_KEY = Constants.expoConfig?.extra?.EXPO_PUBLIC_GROQ_API_KEY 
  || process.env.EXPO_PUBLIC_GROQ_API_KEY;
const GROQ_API_URL = 'https://api.groq.com/openai/v1/chat/completions';

// Backup 2 : OpenAI (Payant)
const OPENAI_API_KEY = Constants.expoConfig?.extra?.EXPO_PUBLIC_OPENAI_API_KEY 
  || process.env.EXPO_PUBLIC_OPENAI_API_KEY;
const OPENAI_API_URL = 'https://api.openai.com/v1/chat/completions';

// Debug: VÃ©rifier les clÃ©s API
console.log('ğŸš€ Gemini API Key prÃ©sente:', GEMINI_API_KEY ? 'Oui (' + GEMINI_API_KEY.substring(0, 10) + '...)' : 'Non');
console.log('ğŸ”‘ Groq API Key prÃ©sente (backup 1):', GROQ_API_KEY ? 'Oui (' + GROQ_API_KEY.substring(0, 10) + '...)' : 'Non');
console.log('ğŸ”‘ OpenAI API Key prÃ©sente (backup 2):', OPENAI_API_KEY ? 'Oui (' + OPENAI_API_KEY.substring(0, 10) + '...)' : 'Non');
console.log('ğŸŒ Environment:', process.env.NODE_ENV);
console.log('ğŸ“± Platform:', Platform?.OS || 'unknown');

// Contexte systÃ¨me pour Wami-IA (multilingue)
const getSystemPrompt = (languageCode) => {
  const languageInstructions = {
    fr: 'RÃ©ponds en franÃ§ais simple et clair.',
    malinke: 'RÃ©ponds en MalinkÃ© (Maninkakan). Utilise les termes techniques en franÃ§ais entre parenthÃ¨ses. Exemple: "Funteni (TempÃ©rature) ye 26Â°C ye."',
    sw: 'Jibu kwa Kiswahili rahisi na wazi.',
    ha: 'Ka amsa da Hausa mai sauÆ™i da bayani.',
    en: 'Respond in simple and clear English.',
    bci: 'RÃ©ponds en BaoulÃ©. Utilise les termes techniques en franÃ§ais entre parenthÃ¨ses si nÃ©cessaire.',
    ar: 'Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø© ÙˆØ§Ù„ÙˆØ§Ø¶Ø­Ø©.',
  };

  return `Tu es Wami-IA, assistante en pisciculture.

RÃˆGLE PRINCIPALE : RÃ©ponds UNIQUEMENT Ã  ce que l'utilisateur demande. Rien de plus.

COMPORTEMENT STRICT :
- Si on te dit "Bonjour" â†’ rÃ©ponds juste "Bonjour ! Comment puis-je t'aider ?". RIEN D'AUTRE.
- Si on te demande la tempÃ©rature â†’ donne SEULEMENT la tempÃ©rature. Ne parle pas du pH, de l'oxygÃ¨ne, etc.
- Si on te demande un conseil â†’ donne UN conseil prÃ©cis. Pas une liste de 10 choses.
- Si on te demande tous les paramÃ¨tres â†’ lÃ  seulement, donne tout.
- N'ajoute JAMAIS d'informations non demandÃ©es.
- Ne rÃ©pÃ¨te JAMAIS ce que tu as dÃ©jÃ  dit.
- Maximum 2-3 phrases par rÃ©ponse.

STYLE :
- Phrases courtes et directes
- Tutoiement
- 1 emoji max par rÃ©ponse
- Pas de listes Ã  puces sauf si demandÃ©

ANALYSE (uniquement si des donnÃ©es sont fournies) :
- TempÃ©rature : OPTIMAL 25-28Â°C | ATTENTION 22-25 ou 28-30Â°C | CRITIQUE <22 ou >30Â°C
- pH : OPTIMAL 7.0-7.5 | ATTENTION 6.5-7.0 ou 7.5-8.0 | CRITIQUE <6.5 ou >8.0
- OxygÃ¨ne : OPTIMAL >6 mg/L | ATTENTION 4-6 | CRITIQUE <4
- Ammoniaque : OPTIMAL <0.25 mg/L | ATTENTION 0.25-0.5 | CRITIQUE >0.5
- TurbiditÃ© : OPTIMAL <20 NTU | ATTENTION 20-30 | CRITIQUE >30
- SalinitÃ© : OPTIMAL 0.3-0.7 ppt | ATTENTION 0.1-0.3 ou 0.7-1.0 | CRITIQUE <0.1 ou >1.0
- Si un paramÃ¨tre est hors norme, signale-le et donne UNE action corrective.

INTERDICTIONS :
1. JAMAIS de format JSON {}
2. JAMAIS d'informations non demandÃ©es
3. JAMAIS de rÃ©pÃ©titions
4. JAMAIS de longs paragraphes

${languageInstructions[languageCode] || languageInstructions.fr}`;
};

/**
 * Appel Ã  Google Gemini (Principal, Gratuit)
 * @param {Array} messages - Historique de conversation
 * @param {string} languageCode - Code de langue
 * @returns {Promise<Object>} RÃ©ponse du LLM
 */
export const callGemini = async (messages, languageCode = 'fr', signal = null) => {
  try {
    if (!GEMINI_API_KEY) {
      throw new Error('ClÃ© API Gemini non configurÃ©e');
    }

    console.log('ğŸš€ Appel Google Gemini (gemini-2.0-flash)...');

    // Convertir les messages au format Gemini
    const geminiContents = [];
    
    // Ajouter le system prompt comme premier message utilisateur contextuel
    const systemPrompt = getSystemPrompt(languageCode);
    
    for (const msg of messages) {
      geminiContents.push({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }],
      });
    }

    const url = `${GEMINI_API_URL}/gemini-2.0-flash:generateContent?key=${GEMINI_API_KEY}`;

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        system_instruction: {
          parts: [{ text: systemPrompt }],
        },
        contents: geminiContents,
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 500,
        },
      }),
      ...(signal ? { signal } : {}),
    });

    console.log('ğŸ“¥ RÃ©ponse Gemini, status:', response.status);

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || 'Erreur API Gemini');
    }

    const data = await response.json();
    console.log('âœ… RÃ©ponse Gemini OK');
    
    const content = data.candidates?.[0]?.content?.parts?.[0]?.text;
    if (!content) {
      throw new Error('RÃ©ponse Gemini vide');
    }

    // Essayer de parser en JSON
    try {
      const parsed = JSON.parse(content);
      return parsed;
    } catch (e) {
      console.log('ğŸ“ RÃ©ponse naturelle reÃ§ue (pas de JSON)');
      
      const suggestions = [];
      const lines = content.split('\n');
      lines.forEach(line => {
        if (line.trim().match(/^[-â€¢]\s+(.+)/)) {
          const suggestion = line.trim().replace(/^[-â€¢]\s+/, '');
          if (suggestion.length > 0 && suggestions.length < 3) {
            suggestions.push(suggestion);
          }
        }
      });
      
      if (suggestions.length === 0) {
        suggestions.push('Voir les dÃ©tails', 'Analyser plus', 'Historique');
      }
      
      return {
        response: content,
        suggestions: suggestions,
      };
    }
  } catch (error) {
    console.error('Erreur lors de l\'appel Ã  Gemini:', error);
    throw error;
  }
};

/**
 * Appel Ã  Groq (Backup 1, Gratuit)
 * @param {Array} messages - Historique de conversation
 * @param {string} languageCode - Code de langue
 * @returns {Promise<Object>} RÃ©ponse du LLM
 */
export const callGroq = async (messages, languageCode = 'fr', signal = null) => {
  try {
    console.log('ğŸš€ Appel Groq avec clÃ©:', GROQ_API_KEY ? GROQ_API_KEY.substring(0, 15) + '...' : 'AUCUNE');
    
    if (!GROQ_API_KEY) {
      throw new Error('ClÃ© API Groq non configurÃ©e');
    }

    console.log('ğŸ“¤ Envoi requÃªte Ã  Groq (Llama 3.3 70B)...');
    
    // Nettoyer les messages pour Ã©viter les rÃ©fÃ©rences circulaires
    const cleanMessages = messages.map(msg => ({
      role: msg.role,
      content: typeof msg.content === 'string' ? msg.content : String(msg.content || '')
    }));
    
    const response = await fetch(GROQ_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${GROQ_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'llama-3.3-70b-versatile', // ModÃ¨le production Groq, excellent multilingue
        messages: [
          { role: 'system', content: getSystemPrompt(languageCode) },
          ...cleanMessages,
        ],
        temperature: 0.7,
        max_tokens: 500,
        // Note: Groq ne supporte pas response_format pour tous les modÃ¨les
      }),
      ...(signal ? { signal } : {}),
    });

    console.log('ğŸ“¥ RÃ©ponse reÃ§ue, status:', response.status);
    
    if (!response.ok) {
      const error = await response.json();
      console.error('âŒ Erreur Groq:', error);
      throw new Error(error.error?.message || 'Erreur API Groq');
    }

    const data = await response.json();
    console.log('âœ… RÃ©ponse Groq OK');
    const content = data.choices[0].message.content;
    
    // Essayer de parser en JSON d'abord (pour compatibilitÃ©)
    try {
      const parsed = JSON.parse(content);
      return parsed;
    } catch (e) {
      // Si ce n'est pas du JSON, c'est une rÃ©ponse naturelle (ce qu'on veut !)
      console.log('ğŸ“ RÃ©ponse naturelle reÃ§ue (pas de JSON)');
      
      // Extraire les suggestions si elles sont prÃ©sentes dans le texte
      const suggestions = [];
      const lines = content.split('\n');
      
      lines.forEach(line => {
        // DÃ©tecter les lignes qui commencent par - ou â€¢ (suggestions)
        if (line.trim().match(/^[-â€¢]\s+(.+)/)) {
          const suggestion = line.trim().replace(/^[-â€¢]\s+/, '');
          if (suggestion.length > 0 && suggestions.length < 3) {
            suggestions.push(suggestion);
          }
        }
      });
      
      // Si pas de suggestions trouvÃ©es, en gÃ©nÃ©rer des gÃ©nÃ©riques
      if (suggestions.length === 0) {
        suggestions.push('Voir les dÃ©tails', 'Analyser plus', 'Historique');
      }
      
      return {
        response: content,
        suggestions: suggestions,
      };
    }
  } catch (error) {
    console.error('Erreur lors de l\'appel Ã  Groq:', error);
    throw error;
  }
};

/**
 * Appel Ã  OpenAI (backup)
 * @param {Array} messages - Historique de conversation
 * @param {string} languageCode - Code de langue
 * @returns {Promise<Object>} RÃ©ponse du LLM
 */
export const callOpenAI = async (messages, languageCode = 'fr', signal = null) => {
  try {
    if (!OPENAI_API_KEY) {
      throw new Error('ClÃ© API OpenAI non configurÃ©e');
    }

    console.log('ğŸš€ Appel OpenAI GPT-4o-mini...');

    // Nettoyer les messages pour Ã©viter les rÃ©fÃ©rences circulaires
    const cleanMessages = messages.map(msg => ({
      role: msg.role,
      content: typeof msg.content === 'string' ? msg.content : String(msg.content || '')
    }));

    const response = await fetch(OPENAI_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: getSystemPrompt(languageCode) },
          ...cleanMessages,
        ],
        temperature: 0.7,
        max_tokens: 500,
      }),
      ...(signal ? { signal } : {}),
    });

    console.log('ğŸ“¥ RÃ©ponse OpenAI, status:', response.status);

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || 'Erreur API OpenAI');
    }

    const data = await response.json();
    console.log('âœ… RÃ©ponse OpenAI OK');
    const content = data.choices[0].message.content;
    
    try {
      const parsed = JSON.parse(content);
      return parsed;
    } catch (e) {
      console.log('ğŸ“ RÃ©ponse naturelle reÃ§ue (pas de JSON)');
      
      const suggestions = [];
      const lines = content.split('\n');
      lines.forEach(line => {
        if (line.trim().match(/^[-â€¢]\s+(.+)/)) {
          const suggestion = line.trim().replace(/^[-â€¢]\s+/, '');
          if (suggestion.length > 0 && suggestions.length < 3) {
            suggestions.push(suggestion);
          }
        }
      });
      
      if (suggestions.length === 0) {
        suggestions.push('Voir les dÃ©tails', 'Analyser plus', 'Historique');
      }
      
      return {
        response: content,
        suggestions: suggestions,
      };
    }
  } catch (error) {
    console.error('Erreur lors de l\'appel Ã  OpenAI:', error);
    throw error;
  }
};

/**
 * Appel Ã  Anthropic Claude (alternative)
 * @param {Array} messages - Historique de conversation
 * @returns {Promise<Object>} RÃ©ponse du LLM
 */
export const callClaude = async (messages) => {
  const ANTHROPIC_API_KEY = Constants.expoConfig?.extra?.EXPO_PUBLIC_ANTHROPIC_API_KEY || process.env.EXPO_PUBLIC_ANTHROPIC_API_KEY;
  const ANTHROPIC_API_URL = 'https://api.anthropic.com/v1/messages';

  try {
    if (!ANTHROPIC_API_KEY || ANTHROPIC_API_KEY === 'your_anthropic_api_key_here') {
      throw new Error('ClÃ© API Anthropic non configurÃ©e');
    }

    const response = await fetch(ANTHROPIC_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': ANTHROPIC_API_KEY,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: 'claude-3-sonnet-20240229',
        max_tokens: 500,
        system: getSystemPrompt('fr'),
        messages: messages,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || 'Erreur API Anthropic');
    }

    const data = await response.json();
    const content = data.content[0].text;
    
    try {
      return JSON.parse(content);
    } catch (e) {
      return {
        response: content,
        suggestions: ['Voir les dÃ©tails', 'Analyser plus', 'Historique'],
      };
    }
  } catch (error) {
    console.error('Erreur lors de l\'appel Ã  Claude:', error);
    throw error;
  }
};

/**
 * Appel Ã  Ollama (local, gratuit)
 * @param {Array} messages - Historique de conversation
 * @returns {Promise<Object>} RÃ©ponse du LLM
 */
export const callOllama = async (messages) => {
  const OLLAMA_URL = 'http://localhost:11434/api/chat';

  try {
    const response = await fetch(OLLAMA_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'llama3', // ou 'mistral', 'codellama', etc.
        messages: [
          { role: 'system', content: getSystemPrompt('fr') },
          ...messages,
        ],
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error('Erreur Ollama. Assurez-vous qu\'Ollama est lancÃ©.');
    }

    const data = await response.json();
    const content = data.message.content;
    
    try {
      return JSON.parse(content);
    } catch (e) {
      return {
        response: content,
        suggestions: ['Voir les dÃ©tails', 'Analyser plus', 'Historique'],
      };
    }
  } catch (error) {
    console.error('Erreur lors de l\'appel Ã  Ollama:', error);
    throw error;
  }
};

/**
 * Fonction principale pour appeler le LLM configurÃ© (multilingue)
 * @param {string} userMessage - Message de l'utilisateur
 * @param {Array} conversationHistory - Historique de conversation
 * @param {string} languageCode - Code de langue (optionnel, dÃ©tectÃ© automatiquement)
 * @param {Object} waterData - DonnÃ©es actuelles de l'eau (optionnel)
 * @returns {Promise<Object>} RÃ©ponse du LLM
 */
export const getLLMResponse = async (userMessage, conversationHistory = [], languageCode = null, waterData = null, signal = null) => {
  // Obtenir la langue actuelle ou dÃ©tecter
  const currentLang = languageCode || getCurrentLanguage();
  
  // Construire l'historique au format attendu
  const messages = conversationHistory.map(msg => ({
    role: msg.sender === 'user' ? 'user' : 'assistant',
    content: msg.text,
  }));

  // Si on a des donnÃ©es d'eau, les inclure dans le contexte (sauf en mode apprentissage)
  let contextualMessage = userMessage;
  
  // VÃ©rifier si c'est le mode apprentissage
  const isLearningMode = userMessage.includes('MODE APPRENTISSAGE') || userMessage.includes('Je suis dÃ©butant en pisciculture');
  
  if (waterData && !isLearningMode) {
    console.log('ğŸŒŠ DonnÃ©es d\'eau transmises Ã  l\'IA:', waterData);
    const waterContext = `
DONNÃ‰ES ACTUELLES DE L'EAU (Ã  utiliser pour ton analyse) :
- TempÃ©rature : ${waterData.temperature}Â°C
- pH : ${waterData.ph}
- OxygÃ¨ne : ${waterData.oxygen} mg/L
- Ammoniaque : ${waterData.ammonia} mg/L
- TurbiditÃ© : ${waterData.turbidity} NTU
- SalinitÃ© : ${waterData.salinity} ppt

IMPORTANT : Utilise CES donnÃ©es exactes dans ta rÃ©ponse, pas des valeurs gÃ©nÃ©riques !

Question de l'utilisateur : ${userMessage}`;
    contextualMessage = waterContext;
  } else if (isLearningMode) {
    console.log('ğŸ“š Mode apprentissage dÃ©tectÃ© - Pas de donnÃ©es d\'eau transmises');
    contextualMessage = userMessage;
  } else {
    console.log('âš ï¸ Aucune donnÃ©e d\'eau transmise Ã  l\'IA');
  }

  // Ajouter le nouveau message
  messages.push({
    role: 'user',
    content: contextualMessage,
  });

  // Appeler Groq en prioritÃ© (gratuit), fallback Gemini, puis OpenAI
  try {
    console.log('ğŸ¯ Tentative avec Groq...');
    return await callGroq(messages, currentLang, signal);
  } catch (groqError) {
    // Si la requÃªte a Ã©tÃ© annulÃ©e, ne pas essayer les fallbacks
    if (groqError.name === 'AbortError') throw groqError;
    console.warn('âš ï¸ Groq Ã©chouÃ©, tentative avec Gemini...', groqError.message);
    
    try {
      return await callGemini(messages, currentLang, signal);
    } catch (geminiError) {
      if (geminiError.name === 'AbortError') throw geminiError;
      console.warn('âš ï¸ Gemini Ã©chouÃ©, tentative avec OpenAI...', geminiError.message);
      try {
        return await callOpenAI(messages, currentLang, signal);
      } catch (openaiError) {
        if (openaiError.name === 'AbortError') throw openaiError;
        console.error('âŒ OpenAI aussi Ã©chouÃ©:', openaiError.message);
      }
    }
    
    // Fallback sur rÃ©ponse locale multilingue si les deux APIs Ã©chouent
    const fallbackMessages = {
      fr: "Je rencontre un problÃ¨me de connexion. Voici ce que je peux vous dire avec mes donnÃ©es locales : Tous vos paramÃ¨tres sont dans les normes optimales.",
      malinke: "N bÉ› gÉ›lÉ›ya sÉ”rÉ” ka kÉ› É²É”gÉ”n cÉ›. Nin ye n bÉ› se ka fÉ” i ye ne ka kunnafoniw la : I ka fÉ›É›n bÉ›É› bÉ› É²uman na.",
      sw: "Nina tatizo la muunganisho. Hii ndiyo ninachoweza kukuambia kutoka kwa data yangu ya ndani: Vipimo vyako vyote viko vizuri.",
      ha: "Ina fuskantar matsala ta haÉ—i. Ga abin da zan iya faÉ—a muku da bayanan gida: Duk abubuwan ku suna daidai.",
      en: "I'm experiencing a connection issue. Here's what I can tell you from my local data: All your parameters are within optimal ranges.",
      ar: "Ø£ÙˆØ§Ø¬Ù‡ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„. Ø¥Ù„ÙŠÙƒ Ù…Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø¥Ø®Ø¨Ø§Ø±Ùƒ Ø¨Ù‡ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠØ©: Ø¬Ù…ÙŠØ¹ Ù…Ø¹Ø§ÙŠÙŠØ±Ùƒ Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø«Ù„Ù‰.",
    };
    
    return {
      response: fallbackMessages[currentLang] || fallbackMessages.fr,
      suggestions: ['RÃ©essayer', 'Voir les donnÃ©es', 'Rapport local'],
    };
  }
};

export default {
  getLLMResponse,
  callGemini,
  callGroq,
  callOpenAI,
  callClaude,
  callOllama,
};